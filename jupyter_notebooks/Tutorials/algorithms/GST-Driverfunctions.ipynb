{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver functions for running Gate Set Tomography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pygsti` package provides multiple levels of abstraction over the core Gate Set Tomography (GST) algorithms.  This  tutorial will show you how to work with pyGSTi's top-level functions for performing GST with a minimial amount of effort.  In order to run the GST protocol there are 3 essential ingredients: 1) data specifing the experimental outcomes, 2) a desired, or \"target\", `Model`, and 3) lists of `Circuit` objects, specifying the operation sequences to use at each successive step in the GST optimization.  The [GST overview tutorial](GST-Overview.ipynb), gave an end-to-end example of how to construct GST circuits, run GST, and generate a report.  This tutorial focus on the second step in more detail; more information about circuit construction and report generation can be found in the [GST circuits tutorial](../objects/advanced/GSTCircuitConstruction.ipynb) and [report generation tutorial](../reporting/ReportGeneration.ipynb).\n",
    "\n",
    "There are several different \"driver routines\" for running GST, and we'll cover in turn:\n",
    "- [`do_long_sequence_gst`](#do_long_sequence_gst) - runs a single instance of GST with \"standard\" circuit lists.\n",
    "- [`do_long_sequence_gst_base`](#do_long_sequence_gst_base) - runs a single instance of GST with custom circuit lists.\n",
    "- [`do_stdpractice_gst`](#do_stdpractice_gst) - runs a multiple instances of GST with \"standard\" circuits based on an `ExplicitOpModel` model.\n",
    "\n",
    "Each function returns a single `pygsti.objects.Results` object (see the [Result object tutorial](../objects/advanced/Results.ipynb), which contains the *single* input `DataSet` and one or more *estimates* (`pygsti.objects.Estimate` objects). \n",
    "\n",
    "Note: The abbreviation **LSGST** (lowercase in function names to follow Python naming conventions) stands for *Long Sequence LinearOperator Set Tomography*, and refers to the more powerful and flexible of GST that utilizes long sequences to find model estimates.  LSGST can be compared to *Linear GST*, or **LGST**, which only uses short sequences and as a result provides much less accurate estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygsti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "First, we set our desired *target model* to be the standard $I$, $X(\\pi/2)$, $Y(\\pi/2)$ model that we've been using in many of these tutorials, and use the standard fiducial and germ sequences needed to generate the GST operation sequences (see the [standard module tutorial](../objects/advanced/StandardModules.ipynb)).  We also specify a list of maximum lengths.  We'll analyze the simulated data generated in the [DataSet tutorial](../objects/DataSet.ipynb), so you'll need to run that tutorial if you haven't already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../tutorial_files/Example_Dataset.txt: 100%\n",
      "Writing cache file (to speed future loads): ../tutorial_files/Example_Dataset.txt.cache\n"
     ]
    }
   ],
   "source": [
    "from pygsti.modelpacks import smq1Q_XYI\n",
    "target_model = smq1Q_XYI.target_model()\n",
    "prep_fiducials, meas_fiducials = smq1Q_XYI.prep_fiducials(), smq1Q_XYI.meas_fiducials()\n",
    "germs = smq1Q_XYI.germs()\n",
    "\n",
    "maxLengths = [1,2,4,8,16,32]\n",
    "\n",
    "ds = pygsti.io.load_dataset(\"../tutorial_files/Example_Dataset.txt\", cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"do_long_sequence_gst\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `do_long_sequence_gst`\n",
    "This driver function finds what is logically a **single GST estimate** given a `DataSet`, a target `Model`, and other parameters.  We say \"logically\" because the returned `Results` object may actually contain multiple related estimates in certain cases.  Most important among the other parameters are the fiducial and germ sequences and list of maximum lengths needed to define a *standard*  (*prep_fiducial + germ^power + meas_fiducial*) set of GST circuit lists.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Circuit Creation ---\n",
      "   784 sequences created\n",
      "--- LGST ---\n",
      "  Singular values of I_tilde (truncating to first 4 of 6) = \n",
      "  4.243708948119547\n",
      "  1.1796799227093193\n",
      "  0.9631622821716201\n",
      "  0.9415276081969872\n",
      "  0.04762148814438346\n",
      "  0.015427853978163288\n",
      "  \n",
      "  Singular values of target I_tilde (truncating to first 4 of 6) = \n",
      "  4.242640687119284\n",
      "  1.4142135623730956\n",
      "  1.4142135623730954\n",
      "  1.4142135623730951\n",
      "  2.5394445830714747e-16\n",
      "  1.118988490269554e-16\n",
      "  \n",
      "--- Iterative MLGST: Iter 1 of 6  92 operation sequences ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Sum of Chi^2 = 53.3695 (92 data params - 44 model params = expected mean of 48; p-value = 0.275479)\n",
      "  Completed in 0.5s\n",
      "  2*Delta(log(L)) = 53.6096\n",
      "  Iteration 1 took 0.5s\n",
      "  \n",
      "--- Iterative MLGST: Iter 2 of 6  168 operation sequences ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Sum of Chi^2 = 121.851 (168 data params - 44 model params = expected mean of 124; p-value = 0.53777)\n",
      "  Completed in 0.6s\n",
      "  2*Delta(log(L)) = 122.297\n",
      "  Iteration 2 took 0.7s\n",
      "  \n",
      "--- Iterative MLGST: Iter 3 of 6  285 operation sequences ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Sum of Chi^2 = 225.524 (285 data params - 44 model params = expected mean of 241; p-value = 0.754923)\n",
      "  Completed in 0.9s\n",
      "  2*Delta(log(L)) = 225.917\n",
      "  Iteration 3 took 1.0s\n",
      "  \n",
      "--- Iterative MLGST: Iter 4 of 6  448 operation sequences ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Sum of Chi^2 = 415.774 (448 data params - 44 model params = expected mean of 404; p-value = 0.332276)\n",
      "  Completed in 1.1s\n",
      "  2*Delta(log(L)) = 416.437\n",
      "  Iteration 4 took 1.3s\n",
      "  \n",
      "--- Iterative MLGST: Iter 5 of 6  616 operation sequences ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Sum of Chi^2 = 569.409 (616 data params - 44 model params = expected mean of 572; p-value = 0.522728)\n",
      "  Completed in 1.6s\n",
      "  2*Delta(log(L)) = 570.275\n",
      "  Iteration 5 took 1.8s\n",
      "  \n",
      "--- Iterative MLGST: Iter 6 of 6  784 operation sequences ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Sum of Chi^2 = 717.34 (784 data params - 44 model params = expected mean of 740; p-value = 0.718291)\n",
      "  Completed in 2.2s\n",
      "  2*Delta(log(L)) = 718.336\n",
      "  Iteration 6 took 2.5s\n",
      "  \n",
      "  Switching to ML objective (last iteration)\n",
      "  --- MLGST ---\n",
      "    Maximum log(L) = 359.076 below upper bound of -1.30744e+06\n",
      "      2*Delta(log(L)) = 718.153 (784 data params - 44 model params = expected mean of 740; p-value = 0.710964)\n",
      "    Completed in 4.7s\n",
      "  2*Delta(log(L)) = 718.153\n",
      "  Final MLGST took 4.7s\n",
      "  \n",
      "Iterative MLGST Total Time: 12.6s\n"
     ]
    }
   ],
   "source": [
    "results = pygsti.do_long_sequence_gst(ds, target_model, prep_fiducials, meas_fiducials, germs, maxLengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary of what's inside a Results object is obtained by printing it\n",
    "(for more examples of how to use a Results object, see the [Results tutorial](../objects/advanced/Results.ipynb))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "----------- pyGSTi ModelEstimateResults Object -----------\n",
      "----------------------------------------------------------\n",
      "\n",
      "How to access my contents:\n",
      "\n",
      " .dataset    -- the DataSet used to generate these results\n",
      "\n",
      " .circuit_lists   -- a dict of Circuit lists w/keys:\n",
      " ---------------------------------------------------------\n",
      "  prep fiducials\n",
      "  effect fiducials\n",
      "  germs\n",
      "  iteration\n",
      "  final\n",
      "  all\n",
      "  iteration delta\n",
      "\n",
      " .circuit_structs   -- a dict of CircuitStructures w/keys:\n",
      " ---------------------------------------------------------\n",
      "  iteration\n",
      "  final\n",
      "\n",
      " .estimates   -- a dictionary of Estimate objects:\n",
      " ---------------------------------------------------------\n",
      "  default\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond the minimum\n",
    "The above example supplies the minimal amount of information required to run the long-sequence GST algorithm.  `do_long_sequence_gst` can be used in a variety of contexts and accepts additional (optional) arguments that affect the way the algorithm is run.  Here we make several remarks regarding alternate or more advanced usage of `do_long_sequence_gst`.\n",
    "\n",
    "- For many of the arguments, you can supply either a filename or a python object (e.g. dataset, target model, operation sequence lists), so if you find yourself loading things from files just to pass them in as arguments, you're probabaly working too hard.\n",
    "\n",
    "- Typically we want to apply certain constraints to a GST optimization.  As mentioned in the model tutorial, the space over which a gate-set estimation is carried out is dictated by the parameterization of the `targetModel` argument.  For example, to constrain a GST estimate to be trace-preserving, one should call `set_all_parameterizations(\"TP\")` on the target `Model` before calling `do_long_sequence_gst`.  See the [tutorial on explicit models](../objects/ExplicitModel.ipynb) for more information.\n",
    "\n",
    "- the `gaugeOptParams` argument specifies a dictionary of parameters ultimately to be passed to the `gaugeopt_to_target` function (which provides full documentation).  By specifying an `itemWeights` argument we can set the ratio of the state preparation and measurement (SPAM) weighting to the gate weighting when performing a gauge optimization.  In the example below, the gate parameters are weighted 1000 times more relative to the SPAM parameters.  Mathematically this corresponds to a multiplicative factor of 0.001 preceding the sum-of-squared-difference terms corresponding to SPAM elements in the model.   Typically it is good to weight the gates parameters more heavily since GST amplifies gate parameter errors via long operation sequences but cannot amplify SPAM parameter errors.  If unsure, 0.001 is a good value to start with.  For more details on the arguments of `gaugeopt_to_target`, see the previous tutorial on low-level algorithms.  For more infomation, see the [gauge optimization tutorial](advanced/GaugeOpt.ipynb).\n",
    "\n",
    "The below call illustrates all three of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Circuit Creation ---\n",
      "   784 sequences created\n",
      "Loading from cache file: ../tutorial_files/Example_Dataset.txt.cache\n",
      "--- LGST ---\n",
      "  Singular values of I_tilde (truncating to first 4 of 6) = \n",
      "  4.243708948119547\n",
      "  1.1796799227093193\n",
      "  0.9631622821716201\n",
      "  0.9415276081969872\n",
      "  0.04762148814438346\n",
      "  0.015427853978163288\n",
      "  \n",
      "  Singular values of target I_tilde (truncating to first 4 of 6) = \n",
      "  4.242640687119284\n",
      "  1.4142135623730956\n",
      "  1.4142135623730954\n",
      "  1.4142135623730951\n",
      "  2.5394445830714747e-16\n",
      "  1.118988490269554e-16\n",
      "  \n",
      "--- Iterative MLGST: Iter 1 of 6  92 operation sequences ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Sum of Chi^2 = 53.5198 (92 data params - 31 model params = expected mean of 61; p-value = 0.740869)\n",
      "  Completed in 0.4s\n",
      "  2*Delta(log(L)) = 53.7539\n",
      "  Iteration 1 took 0.5s\n",
      "  \n",
      "--- Iterative MLGST: Iter 2 of 6  168 operation sequences ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Sum of Chi^2 = 122.392 (168 data params - 31 model params = expected mean of 137; p-value = 0.809296)\n",
      "  Completed in 0.4s\n",
      "  2*Delta(log(L)) = 122.796\n",
      "  Iteration 2 took 0.5s\n",
      "  \n",
      "--- Iterative MLGST: Iter 3 of 6  285 operation sequences ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Sum of Chi^2 = 225.901 (285 data params - 31 model params = expected mean of 254; p-value = 0.897248)\n",
      "  Completed in 0.6s\n",
      "  2*Delta(log(L)) = 226.203\n",
      "  Iteration 3 took 0.7s\n",
      "  \n",
      "--- Iterative MLGST: Iter 4 of 6  448 operation sequences ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Sum of Chi^2 = 416.625 (448 data params - 31 model params = expected mean of 417; p-value = 0.495964)\n",
      "  Completed in 1.0s\n",
      "  2*Delta(log(L)) = 417.12\n",
      "  Iteration 4 took 1.2s\n",
      "  \n",
      "--- Iterative MLGST: Iter 5 of 6  616 operation sequences ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Sum of Chi^2 = 569.869 (616 data params - 31 model params = expected mean of 585; p-value = 0.665202)\n",
      "  Completed in 1.5s\n",
      "  2*Delta(log(L)) = 570.489\n",
      "  Iteration 5 took 1.8s\n",
      "  \n",
      "--- Iterative MLGST: Iter 6 of 6  784 operation sequences ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Sum of Chi^2 = 717.835 (784 data params - 31 model params = expected mean of 753; p-value = 0.816845)\n",
      "  Completed in 2.2s\n",
      "  2*Delta(log(L)) = 718.521\n",
      "  Iteration 6 took 2.6s\n",
      "  \n",
      "  Switching to ML objective (last iteration)\n",
      "  --- MLGST ---\n",
      "    Maximum log(L) = 359.249 below upper bound of -1.30744e+06\n",
      "      2*Delta(log(L)) = 718.499 (784 data params - 31 model params = expected mean of 753; p-value = 0.812125)\n",
      "    Completed in 4.8s\n",
      "  2*Delta(log(L)) = 718.499\n",
      "  Final MLGST took 4.8s\n",
      "  \n",
      "Iterative MLGST Total Time: 12.1s\n"
     ]
    }
   ],
   "source": [
    "mdl_target_TP = target_model.copy() #make a copy so we don't change target_model's parameterization, \n",
    "                                #  since this could be confusing later...\n",
    "mdl_target_TP.set_all_parameterizations(\"TP\") #constrain GST estimate to TP\n",
    "\n",
    "results_TP = pygsti.do_long_sequence_gst(\"../tutorial_files/Example_Dataset.txt\", mdl_target_TP,\n",
    "                                         prep_fiducials, meas_fiducials, germs, maxLengths,\n",
    "                                        gaugeOptParams={'itemWeights': {'gates': 1.0, 'spam': 0.001}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"do_long_sequence_gst_base\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `do_long_sequence_gst_base`\n",
    "This function performs the same analysis as `do_long_sequence_gst` except it allows the user to fully specify the list of operation sequences as either a list of lists of `Circuit` objects or a list of or single `CircuitStructure` object(s). A `CircuitStructure` is preferable as it allows the structured plotting of the sequences in report figures.  In this example, we'll just generate a standard set of structures, but with some of the sequences randomly dropped (see the [tutorial on GST circuit reduction](advanced/GST-FiducialPairReduction.ipynb).  Note that like `do_long_sequence_gst`, `do_long_sequence_gst_base` is able to take filenames as arguments and accepts a `gaugeOptParams` argument for customizing the gauge optimization that is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LGST ---\n",
      "  Singular values of I_tilde (truncating to first 4 of 6) = \n",
      "  4.243708948119547\n",
      "  1.1796799227093193\n",
      "  0.9631622821716201\n",
      "  0.9415276081969872\n",
      "  0.04762148814438346\n",
      "  0.015427853978163288\n",
      "  \n",
      "  Singular values of target I_tilde (truncating to first 4 of 6) = \n",
      "  4.242640687119284\n",
      "  1.4142135623730956\n",
      "  1.4142135623730954\n",
      "  1.4142135623730951\n",
      "  2.5394445830714747e-16\n",
      "  1.118988490269554e-16\n",
      "  \n",
      "--- Iterative MLGST: Iter 1 of 6  92 operation sequences ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Sum of Chi^2 = 53.3695 (92 data params - 44 model params = expected mean of 48; p-value = 0.275479)\n",
      "  Completed in 0.6s\n",
      "  2*Delta(log(L)) = 53.6096\n",
      "  Iteration 1 took 0.6s\n",
      "  \n",
      "--- Iterative MLGST: Iter 2 of 6  132 operation sequences ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Sum of Chi^2 = 97.8315 (132 data params - 44 model params = expected mean of 88; p-value = 0.222143)\n",
      "  Completed in 0.5s\n",
      "  2*Delta(log(L)) = 98.1862\n",
      "  Iteration 2 took 0.5s\n",
      "  \n",
      "--- Iterative MLGST: Iter 3 of 6  197 operation sequences ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Sum of Chi^2 = 151.827 (197 data params - 44 model params = expected mean of 153; p-value = 0.511609)\n",
      "  Completed in 0.7s\n",
      "  2*Delta(log(L)) = 152.108\n",
      "  Iteration 3 took 0.8s\n",
      "  \n",
      "--- Iterative MLGST: Iter 4 of 6  279 operation sequences ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Sum of Chi^2 = 241.67 (279 data params - 44 model params = expected mean of 235; p-value = 0.368629)\n",
      "  Completed in 0.8s\n",
      "  2*Delta(log(L)) = 242.062\n",
      "  Iteration 4 took 0.9s\n",
      "  \n",
      "--- Iterative MLGST: Iter 5 of 6  365 operation sequences ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Sum of Chi^2 = 314.446 (365 data params - 44 model params = expected mean of 321; p-value = 0.592536)\n",
      "  Completed in 1.0s\n",
      "  2*Delta(log(L)) = 315.014\n",
      "  Iteration 5 took 1.2s\n",
      "  \n",
      "--- Iterative MLGST: Iter 6 of 6  451 operation sequences ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Sum of Chi^2 = 392.327 (451 data params - 44 model params = expected mean of 407; p-value = 0.690458)\n",
      "  Completed in 1.5s\n",
      "  2*Delta(log(L)) = 392.939\n",
      "  Iteration 6 took 1.8s\n",
      "  \n",
      "  Switching to ML objective (last iteration)\n",
      "  --- MLGST ---\n",
      "    Maximum log(L) = 196.415 below upper bound of -749451\n",
      "      2*Delta(log(L)) = 392.831 (451 data params - 44 model params = expected mean of 407; p-value = 0.684059)\n",
      "    Completed in 1.1s\n",
      "  2*Delta(log(L)) = 392.831\n",
      "  Final MLGST took 1.2s\n",
      "  \n",
      "Iterative MLGST Total Time: 7.0s\n"
     ]
    }
   ],
   "source": [
    "#Create the same sequences but drop 50% of them randomly for each repeated-germ block.\n",
    "lsgst_structs = pygsti.construction.make_lsgst_structs(target_model, prep_fiducials, meas_fiducials,\n",
    "                                                       germs, maxLengths, keepFraction=0.5, keepSeed=2018)\n",
    "results_reduced = pygsti.do_long_sequence_gst_base(ds, target_model, lsgst_structs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"do_stdpractice_gst\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `do_stdpractice_gst`\n",
    "This function calls `do_long_sequence_gst` multiple times using typical variations in gauge optimization parameters and `ExplicitOpModel` parameterization (this doesn't work for other types `Model` objects, e.g. *implicit* models, which don't implement `set_all_parameterizations`).  This function provides a clean and simple interface to performing a \"usual\" set of GST analyses on a set of data.  As such, it takes a single `DataSet`, similar gate-sequence-specifying parameters to `do_long_sequence_gst`, and a new `modes` argument which is a comma-separated list of \"canned\" GST analysis types (e.g. `\"TP,CPTP\"` will compute a Trace-Preserving estimate *and* a Completely-Positive & Trace-Preserving estimate). The currently available modes are:\n",
    " - \"full\" : unconstrained gates (fully parameterized)                                                                 \n",
    " - \"TP\"   : TP-constrained gates and state preparations\n",
    " - \"CPTP\" : CPTP-constrained gates and TP-constrained state preparations               \n",
    " - \"H+S\"  : Only Hamiltonian and Pauli stochastic errors allowed (CPTP)                                             \n",
    " - \"S\"    : Only Pauli-stochastic errors allowed (CPTP)                                                           \n",
    " - \"Target\" : use the target (ideal) gates as the estimate       \n",
    "\n",
    "The gauge optimization(s) `do_stdpractice_gst` performs are controlled by its `gaugeOptSuite` and `gaugeOptTarget` arguments. The former is can be either a string, specifying a standard \"suite\" of gauge optimizations, or a dictionary of dictionaries similar to the `gaugeOptParams` argument of `do_long_sequence_gst` (see docstring).  The `gaugeOptTarget` argument may be set to a `Model` that is used as the target for gauge optimization, overriding the (typically ideal) target gates given by the `targetModelFilenameOrObj` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Circuit Creation ---\n",
      "   784 sequences created\n",
      "-- Std Practice:  Iter 1 of 3  (TP) --: \n",
      "  --- Iterative MLGST: [##################################################] 100.0%  784 operation sequences ---\n",
      "  Iterative MLGST Total Time: 12.3s\n",
      "-- Std Practice:  Iter 2 of 3  (CPTP) --: \n",
      "  --- Iterative MLGST: [##################################################] 100.0%  784 operation sequences ---\n",
      "  Iterative MLGST Total Time: 31.5s\n",
      "-- Std Practice:  Iter 3 of 3  (Target) --: \n"
     ]
    }
   ],
   "source": [
    "results_stdprac = pygsti.do_stdpractice_gst(ds, target_model, prep_fiducials, meas_fiducials, germs, maxLengths,\n",
    "                                        modes=\"TP,CPTP,Target\") #uses the default suite of gauge-optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimates:  TP, CPTP, Target\n",
      "TP Estimate's gauge optimized models:  stdgaugeopt\n"
     ]
    }
   ],
   "source": [
    "print(\"Estimates: \", \", \".join(results_stdprac.estimates.keys()))\n",
    "print(\"TP Estimate's gauge optimized models: \", \", \".join(results_stdprac.estimates[\"TP\"].goparameters.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll perform the same analysis but with a **non-default standard suite of gauge optimizations** - this one toggles the SPAM penalty in addition to varying the spam weight (the default suite just varies the spam weight without any SPAM penalty).  See the [gauge optimization tutorial](advanced/GaugeOpt.ipynb) for more details on gauge optmization \"suites\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Circuit Creation ---\n",
      "   784 sequences created\n",
      "-- Std Practice:  Iter 1 of 3  (TP) --: \n",
      "  --- Iterative MLGST: [##################################################] 100.0%  784 operation sequences ---\n",
      "  Iterative MLGST Total Time: 12.8s\n",
      "-- Std Practice:  Iter 2 of 3  (CPTP) --: \n",
      "  --- Iterative MLGST: [##################################################] 100.0%  784 operation sequences ---\n",
      "  Iterative MLGST Total Time: 33.3s\n",
      "-- Std Practice:  Iter 3 of 3  (Target) --: \n"
     ]
    }
   ],
   "source": [
    "results_stdprac_nondefaultgo = pygsti.do_stdpractice_gst(\n",
    "    ds, target_model, prep_fiducials, meas_fiducials, germs, maxLengths,\n",
    "    modes=\"TP,CPTP,Target\", gaugeOptSuite=\"varySpam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimates:  TP, CPTP, Target\n",
      "TP Estimate's gauge optimized models:  Spam 0.0001, Spam 0.1, Spam 0.0001+v, Spam 0.1+v\n"
     ]
    }
   ],
   "source": [
    "print(\"Estimates: \", \", \".join(results_stdprac_nondefaultgo.estimates.keys()))\n",
    "print(\"TP Estimate's gauge optimized models: \", \", \".join(results_stdprac_nondefaultgo.estimates[\"TP\"].goparameters.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll demonstrate how to specify a fully custom set of gauge optimization parameters and how to use a **separately-specified target model for gauge optimization**.  You can get a more intuitive gauge-optimized `Model` when by placing as much expected noise as possible into the gauge-optimization target, as this essentially tells the algorithm \"this is what I think the estimated model should look like\".  If you just use the perfect or ideal model for this (the default), then the gauge optimizer may make tradeoffs which don't reflect the expected physics (remember, all gauge-equivalent models product the same observables!).  For example, it may spread error across all your gate operations when you expect just the 2-qubit operations are noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Circuit Creation ---\n",
      "   784 sequences created\n",
      "-- Std Practice:  Iter 1 of 3  (TP) --: \n",
      "  --- Iterative MLGST: [##################################################] 100.0%  784 operation sequences ---\n",
      "  Iterative MLGST Total Time: 10.9s\n",
      "-- Std Practice:  Iter 2 of 3  (CPTP) --: \n",
      "  --- Iterative MLGST: [##################################################] 100.0%  784 operation sequences ---\n",
      "  Iterative MLGST Total Time: 28.5s\n",
      "-- Std Practice:  Iter 3 of 3  (Target) --: \n"
     ]
    }
   ],
   "source": [
    "my_goparams = { 'itemWeights': {'gates': 1.0, 'spam': 0.001} }\n",
    "my_gaugeOptTarget = target_model.depolarize(op_noise=0.005, spam_noise=0.01) # a guess at what estimate should be\n",
    "results_stdprac_customgo = pygsti.do_stdpractice_gst(\n",
    "    ds, target_model, prep_fiducials, meas_fiducials, germs, maxLengths,\n",
    "    modes=\"TP,CPTP,Target\", gaugeOptSuite={ 'myGO': my_goparams }, gaugeOptTarget=my_gaugeOptTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimates:  TP, CPTP, Target\n",
      "TP Estimate's gauge optimized models:  myGO\n"
     ]
    }
   ],
   "source": [
    "print(\"Estimates: \", \", \".join(results_stdprac_customgo.estimates.keys()))\n",
    "print(\"TP Estimate's gauge optimized models: \", \", \".join(results_stdprac_customgo.estimates[\"TP\"].goparameters.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish up, we'll pickle the results for processing in other tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(results, open('../tutorial_files/exampleResults.pkl',\"wb\"))\n",
    "pickle.dump(results_TP, open('../tutorial_files/exampleResults_TP.pkl',\"wb\"))\n",
    "pickle.dump(results_reduced, open('../tutorial_files/exampleResults_reduced.pkl',\"wb\"))\n",
    "pickle.dump(results_stdprac, open('../tutorial_files/exampleResults_stdprac.pkl',\"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
