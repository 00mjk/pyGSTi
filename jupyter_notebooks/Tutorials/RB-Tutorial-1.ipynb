{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import relevant modules\n",
    "import pygsti\n",
    "import rb\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as _np\n",
    "from pygsti.construction import std1Q_XYI\n",
    "from scipy.optimize import curve_fit\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pygsti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pygsti.construction.build_gateset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Declare target gate set (here, we're using standard {I, X(pi/2), Y(pi/2)} gate set)\n",
    "gs_target = std1Q_XYI.gs_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Declare a \"primitives dictionary\".  Here the primitives are the actual\n",
    "#gate operations {I, X(pi/2), Y(pi/2)}.  The Clifford operations are\n",
    "#expressed using the larger gate set of {I, X(pi/2), X(pi), X(-pi/2), Y(pi/2), Y(pi), Y(-pi/2)}.\n",
    "#We call this larger gate set the \"canonical gate set\" and declare here a dictionary\n",
    "#mapping the canonical gate set to primitive gate operations.\n",
    "primD = {'Gi':['Gi'],'Gxp2':['Gx'],'Gxp':['Gx','Gx'],'Gxmp2':['Gx','Gx','Gx'],\n",
    "          'Gyp2':['Gy'],'Gyp':['Gy','Gy'],'Gymp2':['Gy','Gy','Gy']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now let's declare:\n",
    "#m_min- minimum Clifford sequence length\n",
    "#m_max- maximum Clifford sequence length\n",
    "#delta_m- step size between Clifford sequences of different lengths\n",
    "#K_m_sched- Schedule for K_m, that is, how many sequences should be sampled with Clifford length m.\n",
    "#Note that K_m_sched can be a constant or an OrderedDict.\n",
    "\n",
    "#Here we are declaring K_m_sched to be a constant, meaning that K_m is a single constant for all m.\n",
    "#See RB-Tutorial-2 for use of K_m as an OrderedDict.\n",
    "\n",
    "m_min = 1\n",
    "m_max = 1000\n",
    "delta_m = 10\n",
    "K_m_sched = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_m_sched is not an OrderedDict, so Wallman and Flammia error bars are not valid.\n"
     ]
    }
   ],
   "source": [
    "#Now we will generate the list of RB sequences to run (in terms of primitive gate\n",
    "#operations), write them to file, and write \n",
    "#an auxiliary file which keeps track of the Clifford length of each primitive gate sequence.\n",
    "\n",
    "filename_base = 'RB-Tutorial-1_template'\n",
    "rb_seqs, cliff_lens = rb.write_empty_rb_files(filename_base,1,1000,100,K_m_sched,'primitive',primD,seed=0)\n",
    "\n",
    "#Because K_m_sched is a constant, we cannot use Wallman and Flammia error bars; the code warns us\n",
    "#accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#There is now an empty template file \"RB-Tutorial-1_template.txt\".  \n",
    "#For actual physical experiments, this file is to be filled with\n",
    "#experimental data and read in using pygsti.io.load_dataset.\n",
    "\n",
    "#Here we will generate fake data instead and just use the resulting dataset object.\n",
    "\n",
    "#Additionally, there is now a pickle file \"RB-Tutorial-1_template_cliff_seq_lengths.pkl\"\n",
    "#which lists in order the Clifford length of each sequence.\n",
    "#For analyzing experimental data, this pickle file should be read in, \n",
    "#or the above-declared cliff_lens list should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To generate a dataset, we first need to make a gateset.\n",
    "#Here we assume a gate set that is perfect except for some small amount of depolarizing noise on each gate.\n",
    "\n",
    "depol_strength = 1e-3\n",
    "\n",
    "gs_experimental = std1Q_XYI.gs_target\n",
    "gs_experimental = gs_experimental.depolarize(gate_noise=depol_strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14959716797\n"
     ]
    }
   ],
   "source": [
    "#Now we choose the number of clicks per experiment and simulate our data.\n",
    "#Note that time to generate simulated data can be nontrivial.  \n",
    "#Here, it should take about 1 second.\n",
    "\n",
    "N=100\n",
    "\n",
    "start = time.time()\n",
    "rb_data = pygsti.construction.generate_fake_data(gs_experimental,rb_seqs,N,'binomial',seed=1)\n",
    "end = time.time()\n",
    "print end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now it is time to analyze the RB data.\n",
    "#After analyzing the data, an rb_results object is generated.\n",
    "#We extract the relevant parameters we care about from the rb_results object.\n",
    "\n",
    "#We set the following keyword arguments here:\n",
    "#prim_dict = primD.  Declares the predefined dictionary mapping the \"canonical\" gate set to our primitive gate set.\n",
    "#pre_avg = True.  Are we, prior to fitting to the RB decay curve, \"pre-averaging\" the data?  That is, are we\n",
    "#using a single survival probability per sequence length or not?  For now I recommend keeping this set to True.\n",
    "#process_cliff = True.  Whether or not we actually perform the RB analysis for Clifford operations.\n",
    "#process_prim = True.  Whether or not we actually perform the RB analysis for primitive operations.  \n",
    "#NOTE:  RB IS NOT GUARANTEED TO GIVE RELIABLE PRIMITIVE ANALYSIS (and in general will not!).\n",
    "\n",
    "rb_results = rb.process_rb_data(rb_data,rb_seqs,cliff_lens,prim_dict=primD,pre_avg=True,process_prim=True,process_cliff=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Cliffords:\n",
      "A = 0.503352244485\n",
      "B = 0.499714148643\n",
      "f = 0.99651070056\n",
      "F_avg = 0.99825535028\n",
      "r = 0.00174464971977\n",
      "\n",
      "For primitives:\n",
      "A = 0.509591163322\n",
      "B = 0.492912929923\n",
      "f = 0.998988508108\n",
      "F_avg = 0.999494254054\n",
      "r = 0.000505745945951\n"
     ]
    }
   ],
   "source": [
    "#Let's look at the RB results.  The parameters are defined as follows, following Wallman and Flammia \n",
    "#(http://iopscience.iop.org/article/10.1088/1367-2630/16/10/103032))\n",
    "\n",
    "#A,B,f are fit parameters to the function F(m) = A+B*f^m, where F(m) is the survival probability for sequences of length m.\n",
    "#F_avg = ((d-1)*f+1.)/d, where for n=1 qubit, d=2.  F_avg is the average (Clifford or primitive) gate fidelity.\n",
    "#r = 1-F_avg.  For Cliffords, r is the \"RB number.\"\n",
    "rb_results.print_cliff()\n",
    "print\n",
    "rb_results.print_prim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytic RB error rate: 0.00176791958146\n",
      "Experimental RB error rate: 0.00174464971977\n"
     ]
    }
   ],
   "source": [
    "#Because we generated our data from a known Markovian gate set, we can analytically compute\n",
    "#the Clifford RB error rate r.\n",
    "\n",
    "#First we make map our experimental gate set of primitive operations into a dictionary of Cliffords:\n",
    "\n",
    "gs_cliff_experimental = rb.makeRealCliffs_gs(gs_experimental,primD)\n",
    "\n",
    "#Then we directly compute the average twirled Clifford error rate:\n",
    "\n",
    "analytic_rb_error_rate = rb.analytic_rb_cliff_gateset_error_rate(gs_cliff_experimental)\n",
    "\n",
    "print \"Analytic RB error rate:\", analytic_rb_error_rate\n",
    "print \"Experimental RB error rate:\", rb_results.cliff_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "Generating non-parametric dataset.\n",
      "prim A = 0.509591163322 +/- 0.0154254630407\n",
      "prim B = 0.492912929923 +/- 0.0134503059807\n",
      "prim f = 0.998988508108 +/- 9.49333182931e-05\n",
      "prim F_avg = 0.999494254054 +/- 4.74666591465e-05\n",
      "prim r = 0.000505745945951 +/- 4.74666591465e-05\n",
      "Cliff A = 0.503352244485 +/- 0.0189996303346\n",
      "Cliff B = 0.499714148643 +/- 0.0171342653647\n",
      "Cliff f = 0.99651070056 +/- 0.000331354895069\n",
      "Cliff F_avg = 0.99825535028 +/- 0.000165677447534\n",
      "Cliff r = 0.00174464971977 +/- 0.000165677447534\n"
     ]
    }
   ],
   "source": [
    "#Lastly, let's put some error bars on the estimates.\n",
    "#Because we used a constant K_m_sched, we can't compute analytic error bars using the Wallman and Flammia method.\n",
    "#We can instead, however, compute bootstrapped error bars.\n",
    "#Error bars here are 1-sigma confidence intervals.\n",
    "\n",
    "rb_results.error_bars('bootstrap',process_cliff=True,process_prim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prim A = 0.509591163322 +/- 0.0154254630407\n",
      "prim B = 0.492912929923 +/- 0.0134503059807\n",
      "prim f = 0.998988508108 +/- 9.49333182931e-05\n",
      "prim F_avg = 0.999494254054 +/- 4.74666591465e-05\n",
      "prim r = 0.000505745945951 +/- 4.74666591465e-05\n",
      "Cliff A = 0.503352244485 +/- 0.0189996303346\n",
      "Cliff B = 0.499714148643 +/- 0.0171342653647\n",
      "Cliff f = 0.99651070056 +/- 0.000331354895069\n",
      "Cliff F_avg = 0.99825535028 +/- 0.000165677447534\n",
      "Cliff r = 0.00174464971977 +/- 0.000165677447534\n"
     ]
    }
   ],
   "source": [
    "#Now that we've already generated bootstrapped error bars, we can get them again without having to recalculate them:\n",
    "rb_results.error_bars('bootstrap',process_cliff=True,process_prim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000165677447534\n"
     ]
    }
   ],
   "source": [
    "#We can also manually extract the error bars now; for example:\n",
    "\n",
    "print rb_results.cliff_r_error_BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
