{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Algorithms: high-level interface\n",
    "## (Running GST using Driver Routines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The `pygsti` package provides multiple levels of abstraction over the core Gate Set Tomography (GST) algorithms.  This  tutorial will show you how to work with `pygsti`'s highest level of abstraction, called \"driver functions\" to run GST algorithms with a minimial amount of effort.  In order to run a GST algorithm there are 3 essential ingredients: 1) data specifing the experimental outcomes, 2) a desired, or \"target\", `GateSet`, and 3) multiple lists of gate sequences, specifying the gate sequences to use at each successive step in the GST optimization.  There are currently only a few driver routines, which we'll cover in turn.  Each driver function returns a single `pygsti.objects.Results` object, which contains the *single* input `DataSet` and one or more *estimates* (`pygsti.objects.Estimate` objects). \n",
    "\n",
    "[The abbreviation \"LSGST\" (lowercase in function names to follow Python naming conventions) stands for \"Long Sequence Gate Set Tomography\", and refers to the more powerful flavor of GST that utilizes long sequences to find gate set estimates.  LSGST can be compared to Linear GST, or \"LGST\", which only uses short sequences and as a result provides much less accurate estimates.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pygsti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Setup\n",
    "First, we set our desired \"target gateset\" to be the standard I, X, Y gate set that we've been using throughout the tutorials, and pull in the fiducial and germ sequences needed to generate the GST gate sequences.  We also specify a list of maximum lengths.  We'll analyze the simulated data generated in the data sets tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache file: tutorial_files/Example_Dataset.txt.cache\n"
     ]
    }
   ],
   "source": [
    "from pygsti.construction import std1Q_XYI\n",
    "gs_target = std1Q_XYI.gs_target\n",
    "prep_fiducials, meas_fiducials = std1Q_XYI.prepStrs, std1Q_XYI.effectStrs\n",
    "germs = std1Q_XYI.germs\n",
    "\n",
    "maxLengths = [1,2,4,8,16,32]\n",
    "\n",
    "ds = pygsti.io.load_dataset(\"tutorial_files/Example_Dataset.txt\", cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## `do_long_sequence_gst`\n",
    "This driver function finds what is logically a **single GST estimate** given a `DataSet`, a target `GateSet`, and other parameters.  We say \"logically\" because the returned `Results` object may actually contain multiple related estimates in certain cases.  Most important among the other parameters are the fiducial and germ sequences and list of maximum lengths needed to define a *standard* set of GST gate sequence lists.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Gate Sequence Creation ---\n",
      " 1585 sequences created\n",
      " Dataset has 3121 entries: 1585 utilized, 0 requested sequences were missing\n",
      "--- LGST ---\n",
      "  Singular values of I_tilde (truncating to first 4 of 6) = \n",
      "  4.24503058336\n",
      "  1.17971057338\n",
      "  0.956497891831\n",
      "  0.942353526676\n",
      "  0.0470890214285\n",
      "  0.0153149329552\n",
      "  \n",
      "  Singular values of target I_tilde (truncating to first 4 of 6) = \n",
      "  4.24264068712\n",
      "  1.41421356237\n",
      "  1.41421356237\n",
      "  1.41421356237\n",
      "  3.03015102482e-16\n",
      "  7.09307430818e-17\n",
      "  \n",
      "--- Iterative MLGST: Iter 1 of 6  92 gate strings ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Created evaluation tree with 1 subtrees.  Will divide 1 procs into 1 (subtree-processing)\n",
      "   groups of ~1 procs each, to distribute over 56 params (taken as 1 param groups of ~56 params).\n",
      "  Sum of Chi^2 = 40.9959 (92 data params - 40 model params = expected mean of 52; p-value = 0.864234)\n",
      "  Completed in 0.1s\n",
      "  2*Delta(log(L)) = 41.1735\n",
      "  Iteration 1 took 0.1s\n",
      "  \n",
      "--- Iterative MLGST: Iter 2 of 6  168 gate strings ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Created evaluation tree with 1 subtrees.  Will divide 1 procs into 1 (subtree-processing)\n",
      "   groups of ~1 procs each, to distribute over 56 params (taken as 1 param groups of ~56 params).\n",
      "  Sum of Chi^2 = 129.667 (168 data params - 40 model params = expected mean of 128; p-value = 0.442199)\n",
      "  Completed in 0.1s\n",
      "  2*Delta(log(L)) = 130.105\n",
      "  Iteration 2 took 0.1s\n",
      "  \n",
      "--- Iterative MLGST: Iter 3 of 6  441 gate strings ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Created evaluation tree with 1 subtrees.  Will divide 1 procs into 1 (subtree-processing)\n",
      "   groups of ~1 procs each, to distribute over 56 params (taken as 1 param groups of ~56 params).\n",
      "  Sum of Chi^2 = 428.925 (441 data params - 40 model params = expected mean of 401; p-value = 0.161707)\n",
      "  Completed in 0.2s\n",
      "  2*Delta(log(L)) = 429.811\n",
      "  Iteration 3 took 0.2s\n",
      "  \n",
      "--- Iterative MLGST: Iter 4 of 6  817 gate strings ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Created evaluation tree with 1 subtrees.  Will divide 1 procs into 1 (subtree-processing)\n",
      "   groups of ~1 procs each, to distribute over 56 params (taken as 1 param groups of ~56 params).\n",
      "  Sum of Chi^2 = 774.75 (817 data params - 40 model params = expected mean of 777; p-value = 0.516038)\n",
      "  Completed in 0.3s\n",
      "  2*Delta(log(L)) = 776.451\n",
      "  Iteration 4 took 0.3s\n",
      "  \n",
      "--- Iterative MLGST: Iter 5 of 6  1201 gate strings ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Created evaluation tree with 1 subtrees.  Will divide 1 procs into 1 (subtree-processing)\n",
      "   groups of ~1 procs each, to distribute over 56 params (taken as 1 param groups of ~56 params).\n",
      "  Sum of Chi^2 = 1154.84 (1201 data params - 40 model params = expected mean of 1161; p-value = 0.545507)\n",
      "  Completed in 0.4s\n",
      "  2*Delta(log(L)) = 1156.93\n",
      "  Iteration 5 took 0.5s\n",
      "  \n",
      "--- Iterative MLGST: Iter 6 of 6  1585 gate strings ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Created evaluation tree with 1 subtrees.  Will divide 1 procs into 1 (subtree-processing)\n",
      "   groups of ~1 procs each, to distribute over 56 params (taken as 1 param groups of ~56 params).\n",
      "  Sum of Chi^2 = 1600.78 (1585 data params - 40 model params = expected mean of 1545; p-value = 0.157791)\n",
      "  Completed in 0.6s\n",
      "  2*Delta(log(L)) = 1603.26\n",
      "  Iteration 6 took 0.8s\n",
      "  \n",
      "  Switching to ML objective (last iteration)\n",
      "  --- MLGST ---\n",
      "  Created evaluation tree with 1 subtrees.  Will divide 1 procs into 1 (subtree-processing)\n",
      "   groups of ~1 procs each, to distribute over 56 params (taken as 1 param groups of ~56 params).\n",
      "    Maximum log(L) = 801.394 below upper bound of -2.65022e+06\n",
      "      2*Delta(log(L)) = 1602.79 (1585 data params - 40 model params = expected mean of 1545; p-value = 0.149439)\n",
      "    Completed in 0.7s\n",
      "  2*Delta(log(L)) = 1602.79\n",
      "  Final MLGST took 0.7s\n",
      "  \n",
      "Iterative MLGST Total Time: 2.7s\n"
     ]
    }
   ],
   "source": [
    "results = pygsti.do_long_sequence_gst(ds, gs_target, prep_fiducials, meas_fiducials, germs, maxLengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "---------------- pyGSTi Results Object -------------------\n",
      "----------------------------------------------------------\n",
      "\n",
      "How to access my contents:\n",
      "\n",
      " .dataset    -- the DataSet used to generate these results\n",
      "\n",
      " .gatestring_lists   -- a dict of GateString lists w/keys:\n",
      " ---------------------------------------------------------\n",
      "  iteration\n",
      "  final\n",
      "  all\n",
      "  iteration delta\n",
      "  prep fiducials\n",
      "  effect fiducials\n",
      "  germs\n",
      "\n",
      " .gatestring_structs   -- a dict of GatestringStructures w/keys:\n",
      " ---------------------------------------------------------\n",
      "  iteration\n",
      "  final\n",
      "\n",
      " .estimates   -- a dictionary of Estimate objects:\n",
      " ---------------------------------------------------------\n",
      "  default\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A summary of what's inside a Results object is obtained by printing it\n",
    "# (for more examples of how to use a Results object, see the Results tutorial)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Beynond the minimum\n",
    "The above example supplies the minimal amount of information required to run the long-sequence GST algorithm.  `do_long_sequence_gst` can be used in a variety of contexts and accepts additional (optional) arguments that affect the way the algorithm is run.  Here we make several remarks regarding alternate or more advanced usage of `do_long_sequence_gst`.\n",
    "\n",
    "- For many of the arguments, you can supply either a filename or a python object (e.g. dataset, target gateset, gate string lists), so if you find yourself loading things from files just to pass them in as arguments, you're probabaly working too hard.\n",
    "\n",
    "- Typically we want to apply certain constraints to a GST optimization.  As mentioned in the gate set tutorial, the space over which a gate-set estimation is carried out is dictated by the parameterization of the `targetGateset` argument.  For example, to constrain a GST estimate to be trace-preserving, one should call `set_all_parameterizations(\"TP\")` on the target `GateSet` before calling `do_long_sequence_gst`.\n",
    "\n",
    "- the `gaugeOptParams` argument specifies a dictionary of parameters ultimately to be passed to the `gaugeopt_to_target` function (which provides full documentation).  By specifying an `itemWeights` argument we can set the ratio of the state preparation and measurement (SPAM) weighting to the gate weighting when performing a gauge optimization.  In the example below, the gate parameters are weighted 1000 times more relative to the SPAM parameters.  Mathematically this corresponds to a multiplicative factor of 0.001 preceding the sum-of-squared-difference terms corresponding to SPAM elements in the gateset.   Typically it is good to weight the gates parameters more heavily since GST amplifies gate parameter errors via long gate sequences but cannot amplify SPAM parameter errors.  If unsure, 0.001 is a good value to start with.  For more details on the arguments of `gaugeopt_to_target`, see the previous tutorial on low-level algorithms.\n",
    "\n",
    "The below call illustrates all three of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache file: tutorial_files/Example_Dataset.txt.cache\n",
      "--- Gate Sequence Creation ---\n",
      " 1585 sequences created\n",
      " Dataset has 3121 entries: 1585 utilized, 0 requested sequences were missing\n",
      "Loading from cache file: tutorial_files/Example_Dataset.txt.cache\n",
      "--- LGST ---\n",
      "  Singular values of I_tilde (truncating to first 4 of 6) = \n",
      "  4.24503058336\n",
      "  1.17971057338\n",
      "  0.956497891831\n",
      "  0.942353526676\n",
      "  0.0470890214285\n",
      "  0.0153149329552\n",
      "  \n",
      "  Singular values of target I_tilde (truncating to first 4 of 6) = \n",
      "  4.24264068712\n",
      "  1.41421356237\n",
      "  1.41421356237\n",
      "  1.41421356237\n",
      "  3.03015102482e-16\n",
      "  7.09307430818e-17\n",
      "  \n",
      "--- Iterative MLGST: Iter 1 of 6  92 gate strings ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Created evaluation tree with 1 subtrees.  Will divide 1 procs into 1 (subtree-processing)\n",
      "   groups of ~1 procs each, to distribute over 43 params (taken as 1 param groups of ~43 params).\n",
      "  Sum of Chi^2 = 41.0771 (92 data params - 31 model params = expected mean of 61; p-value = 0.976519)\n",
      "  Completed in 0.0s\n",
      "  2*Delta(log(L)) = 41.2329\n",
      "  Iteration 1 took 0.1s\n",
      "  \n",
      "--- Iterative MLGST: Iter 2 of 6  168 gate strings ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Created evaluation tree with 1 subtrees.  Will divide 1 procs into 1 (subtree-processing)\n",
      "   groups of ~1 procs each, to distribute over 43 params (taken as 1 param groups of ~43 params).\n",
      "  Sum of Chi^2 = 130.16 (168 data params - 31 model params = expected mean of 137; p-value = 0.64796)\n",
      "  Completed in 0.1s\n",
      "  2*Delta(log(L)) = 130.52\n",
      "  Iteration 2 took 0.1s\n",
      "  \n",
      "--- Iterative MLGST: Iter 3 of 6  441 gate strings ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Created evaluation tree with 1 subtrees.  Will divide 1 procs into 1 (subtree-processing)\n",
      "   groups of ~1 procs each, to distribute over 43 params (taken as 1 param groups of ~43 params).\n",
      "  Sum of Chi^2 = 429.293 (441 data params - 31 model params = expected mean of 410; p-value = 0.246157)\n",
      "  Completed in 0.2s\n",
      "  2*Delta(log(L)) = 429.948\n",
      "  Iteration 3 took 0.2s\n",
      "  \n",
      "--- Iterative MLGST: Iter 4 of 6  817 gate strings ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Created evaluation tree with 1 subtrees.  Will divide 1 procs into 1 (subtree-processing)\n",
      "   groups of ~1 procs each, to distribute over 43 params (taken as 1 param groups of ~43 params).\n",
      "  Sum of Chi^2 = 776.117 (817 data params - 31 model params = expected mean of 786; p-value = 0.59231)\n",
      "  Completed in 0.2s\n",
      "  2*Delta(log(L)) = 777.474\n",
      "  Iteration 4 took 0.3s\n",
      "  \n",
      "--- Iterative MLGST: Iter 5 of 6  1201 gate strings ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Created evaluation tree with 1 subtrees.  Will divide 1 procs into 1 (subtree-processing)\n",
      "   groups of ~1 procs each, to distribute over 43 params (taken as 1 param groups of ~43 params).\n",
      "  Sum of Chi^2 = 1156.54 (1201 data params - 31 model params = expected mean of 1170; p-value = 0.604685)\n",
      "  Completed in 0.3s\n",
      "  2*Delta(log(L)) = 1158.12\n",
      "  Iteration 5 took 0.4s\n",
      "  \n",
      "--- Iterative MLGST: Iter 6 of 6  1585 gate strings ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Created evaluation tree with 1 subtrees.  Will divide 1 procs into 1 (subtree-processing)\n",
      "   groups of ~1 procs each, to distribute over 43 params (taken as 1 param groups of ~43 params).\n",
      "  Sum of Chi^2 = 1602.65 (1585 data params - 31 model params = expected mean of 1554; p-value = 0.190603)\n",
      "  Completed in 0.4s\n",
      "  2*Delta(log(L)) = 1604.42\n",
      "  Iteration 6 took 0.5s\n",
      "  \n",
      "  Switching to ML objective (last iteration)\n",
      "  --- MLGST ---\n",
      "  Created evaluation tree with 1 subtrees.  Will divide 1 procs into 1 (subtree-processing)\n",
      "   groups of ~1 procs each, to distribute over 43 params (taken as 1 param groups of ~43 params).\n",
      "    Maximum log(L) = 802.182 below upper bound of -2.65022e+06\n",
      "      2*Delta(log(L)) = 1604.36 (1585 data params - 31 model params = expected mean of 1554; p-value = 0.182536)\n",
      "    Completed in 0.3s\n",
      "  2*Delta(log(L)) = 1604.36\n",
      "  Final MLGST took 0.3s\n",
      "  \n",
      "Iterative MLGST Total Time: 1.8s\n"
     ]
    }
   ],
   "source": [
    "gs_target_TP = gs_target.copy() #make a copy so we don't change gs_target's parameterization, \n",
    "                                #  since this could be confusing later...\n",
    "gs_target_TP.set_all_parameterizations(\"TP\") #constrain GST estimate to TP\n",
    "\n",
    "results_TP = pygsti.do_long_sequence_gst(\"tutorial_files/Example_Dataset.txt\", gs_target_TP,\n",
    "                                         prep_fiducials, meas_fiducials, germs, maxLengths,\n",
    "                                        gaugeOptParams={'itemWeights': {'gates': 1.0, 'spam': 0.001}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## `do_long_sequence_gst_base`\n",
    "This performs the same analysis as `do_long_sequence_gst` except it allows the user to fully specify the list of gate sequences as either a list of lists of `GateString` objects or a list of `GateStringStructure` objects (the latter allow the structured plotting of the sequences in report figures).  In this example, we'll just generate a standard set of structures, but with some of the sequences randomly dropped (see later tutorials on gate string reduction).  Note that like `do_long_sequence_gst`, `do_long_sequence_gst_base` is able to take filenames as arguments and accepts a `gaugeOptParams` argument for customizing the gauge optimization that is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LGST ---\n",
      "  Singular values of I_tilde (truncating to first 4 of 6) = \n",
      "  4.24503058336\n",
      "  1.17971057338\n",
      "  0.956497891831\n",
      "  0.942353526676\n",
      "  0.0470890214285\n",
      "  0.0153149329552\n",
      "  \n",
      "  Singular values of target I_tilde (truncating to first 4 of 6) = \n",
      "  4.24264068712\n",
      "  1.41421356237\n",
      "  1.41421356237\n",
      "  1.41421356237\n",
      "  3.03015102482e-16\n",
      "  7.09307430818e-17\n",
      "  \n",
      "--- Iterative MLGST: Iter 1 of 6  92 gate strings ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Created evaluation tree with 1 subtrees.  Will divide 1 procs into 1 (subtree-processing)\n",
      "   groups of ~1 procs each, to distribute over 56 params (taken as 1 param groups of ~56 params).\n",
      "  Sum of Chi^2 = 40.9959 (92 data params - 40 model params = expected mean of 52; p-value = 0.864234)\n",
      "  Completed in 0.1s\n",
      "  2*Delta(log(L)) = 41.1735\n",
      "  Iteration 1 took 0.1s\n",
      "  \n",
      "--- Iterative MLGST: Iter 2 of 6  133 gate strings ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Created evaluation tree with 1 subtrees.  Will divide 1 procs into 1 (subtree-processing)\n",
      "   groups of ~1 procs each, to distribute over 56 params (taken as 1 param groups of ~56 params).\n",
      "  Sum of Chi^2 = 95.926 (133 data params - 40 model params = expected mean of 93; p-value = 0.397001)\n",
      "  Completed in 0.1s\n",
      "  2*Delta(log(L)) = 96.3219\n",
      "  Iteration 2 took 0.1s\n",
      "  \n",
      "--- Iterative MLGST: Iter 3 of 6  287 gate strings ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Created evaluation tree with 1 subtrees.  Will divide 1 procs into 1 (subtree-processing)\n",
      "   groups of ~1 procs each, to distribute over 56 params (taken as 1 param groups of ~56 params).\n",
      "  Sum of Chi^2 = 273.575 (287 data params - 40 model params = expected mean of 247; p-value = 0.118)\n",
      "  Completed in 0.1s\n",
      "  2*Delta(log(L)) = 274.279\n",
      "  Iteration 3 took 0.2s\n",
      "  \n",
      "--- Iterative MLGST: Iter 4 of 6  478 gate strings ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Created evaluation tree with 1 subtrees.  Will divide 1 procs into 1 (subtree-processing)\n",
      "   groups of ~1 procs each, to distribute over 56 params (taken as 1 param groups of ~56 params).\n",
      "  Sum of Chi^2 = 447.086 (478 data params - 40 model params = expected mean of 438; p-value = 0.371689)\n",
      "  Completed in 0.2s\n",
      "  2*Delta(log(L)) = 448.008\n",
      "  Iteration 4 took 0.2s\n",
      "  \n",
      "--- Iterative MLGST: Iter 5 of 6  672 gate strings ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Created evaluation tree with 1 subtrees.  Will divide 1 procs into 1 (subtree-processing)\n",
      "   groups of ~1 procs each, to distribute over 56 params (taken as 1 param groups of ~56 params).\n",
      "  Sum of Chi^2 = 622.688 (672 data params - 40 model params = expected mean of 632; p-value = 0.596553)\n",
      "  Completed in 0.3s\n",
      "  2*Delta(log(L)) = 623.703\n",
      "  Iteration 5 took 0.3s\n",
      "  \n",
      "--- Iterative MLGST: Iter 6 of 6  867 gate strings ---: \n",
      "  --- Minimum Chi^2 GST ---\n",
      "  Created evaluation tree with 1 subtrees.  Will divide 1 procs into 1 (subtree-processing)\n",
      "   groups of ~1 procs each, to distribute over 56 params (taken as 1 param groups of ~56 params).\n",
      "  Sum of Chi^2 = 836.062 (867 data params - 40 model params = expected mean of 827; p-value = 0.405789)\n",
      "  Completed in 0.4s\n",
      "  2*Delta(log(L)) = 837.23\n",
      "  Iteration 6 took 0.4s\n",
      "  \n",
      "  Switching to ML objective (last iteration)\n",
      "  --- MLGST ---\n",
      "  Created evaluation tree with 1 subtrees.  Will divide 1 procs into 1 (subtree-processing)\n",
      "   groups of ~1 procs each, to distribute over 56 params (taken as 1 param groups of ~56 params).\n",
      "    Maximum log(L) = 418.494 below upper bound of -1.44634e+06\n",
      "      2*Delta(log(L)) = 836.988 (867 data params - 40 model params = expected mean of 827; p-value = 0.397054)\n",
      "    Completed in 0.6s\n",
      "  2*Delta(log(L)) = 836.988\n",
      "  Final MLGST took 0.6s\n",
      "  \n",
      "Iterative MLGST Total Time: 1.9s\n"
     ]
    }
   ],
   "source": [
    "#Create the same sequences but drop 50% of them randomly for each repeated-germ block.\n",
    "lsgst_structs = pygsti.construction.make_lsgst_structs(gs_target, prep_fiducials, meas_fiducials,\n",
    "                                                       germs, maxLengths, keepFraction=0.5, keepSeed=2018)\n",
    "results_reduced = pygsti.do_long_sequence_gst_base(ds, gs_target, lsgst_structs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## `do_std_practice_gst`\n",
    "This driver function calls `do_long_sequence_gst` multiple times using typical variations in gauge optimization parameters and `GateSet` parameterization.  This function provides a clean and simple interface to performing a \"usual\" set of GST analyses on a set of data.  As such, it takes a single `DataSet`, similar gate-sequence-specifying parameters to `do_long_sequence_gst`, and a new `modes` argument which is a comma-separated list of \"canned\" GST analysis types (e.g. `\"TP,CPTP\"` will compute a Trace-Preserving estimate *and* a Completely-Positive & Trace-Preserving estimate). The currently available modes are:\n",
    " - \"full\" : unconstrained gates (fully parameterized)                                                                 \n",
    " - \"TP\"   : TP-constrained gates and state preparations                                                                               \n",
    " - \"CPTP\" : CPTP-constrained gates and TP-constrained state preparations                                                                  \n",
    " - \"H+S\"  : Only Hamiltonian and Pauli stochastic errors allowed (CPTP)                                             \n",
    " - \"S\"    : Only Pauli-stochastic errors allowed (CPTP)                                                           \n",
    " - \"Target\" : use the target (ideal) gates as the estimate       \n",
    "\n",
    "The gauge optimization(s) `do_std_practice_gst` performs are controlled by its `gaugeOptSuite` and `gaugeOptTarget` arguments. The former is can be either a string, specifying a standard \"suite\" of gauge optimizations, or a dictionary of dictionaries similar to the `gaugeOptParams` argument of `do_long_sequence_gst` (see docstring).  The `gaugeOptTarget` argument may be set to a `GateSet` that is used as the target for gauge optimization, overriding the (typically ideal) target gates given by the `targetGateFilenameOrSet` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Std Practice:  Iter 1 of 3  (TP) --: \n",
      "  --- Gate Sequence Creation ---\n",
      "   1585 sequences created\n",
      "   Dataset has 3121 entries: 1585 utilized, 0 requested sequences were missing\n",
      "  --- LGST ---\n",
      "  --- Iterative MLGST: [##################################################] 100.0%  1585 gate strings ---\n",
      "  Iterative MLGST Total Time: 1.8s\n",
      "  -- Performing 'Spam 0.0001' gauge optimization on TP estimate --\n",
      "  -- Done gauge optimizing (0.0325s) -- \n",
      "  -- Performing 'Spam 0.1' gauge optimization on TP estimate --\n",
      "  -- Done gauge optimizing (0.0328512s) -- \n",
      "-- Std Practice:  Iter 2 of 3  (CPTP) --: \n",
      "  --- Gate Sequence Creation ---\n",
      "   1585 sequences created\n",
      "   Dataset has 3121 entries: 1585 utilized, 0 requested sequences were missing\n",
      "  --- Iterative MLGST: [##################################################] 100.0%  1585 gate strings ---\n",
      "  Iterative MLGST Total Time: 5.6s\n",
      "  -- Performing 'Spam 0.0001' gauge optimization on CPTP estimate --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: MLGST failed to improve logl: retaining chi2-objective estimate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -- Done gauge optimizing (0.315197s) -- \n",
      "  -- Performing 'Spam 0.0001' gauge optimization on CPTP.robust estimate --\n",
      "  -- Done gauge optimizing (0.312007s) --\n",
      "  -- Performing 'Spam 0.1' gauge optimization on CPTP estimate --\n",
      "  -- Done gauge optimizing (0.311549s) -- \n",
      "  -- Performing 'Spam 0.1' gauge optimization on CPTP.robust estimate --\n",
      "  -- Done gauge optimizing (0.305241s) --\n",
      "-- Std Practice:  Iter 3 of 3  (Target) --: \n",
      "  -- Performing 'Spam 0.0001' gauge optimization on Target estimate --\n",
      "  -- Done gauge optimizing (0.00191808s) -- \n",
      "  -- Performing 'Spam 0.1' gauge optimization on Target estimate --\n",
      "  -- Done gauge optimizing (0.00197792s) -- \n"
     ]
    }
   ],
   "source": [
    "results_stdprac = pygsti.do_stdpractice_gst(ds, gs_target, prep_fiducials, meas_fiducials, germs, maxLengths,\n",
    "                                        modes=\"TP,CPTP,Target\") #uses the default suite of gauge-optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimates:  TP, CPTP, CPTP.robust, Target\n",
      "TP Estimate's gauge optimized gate sets:  Spam 0.0001, Spam 0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Estimates: \", \", \".join(results_stdprac.estimates.keys()))\n",
    "print(\"TP Estimate's gauge optimized gate sets: \", \", \".join(results_stdprac.estimates[\"TP\"].goparameters.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll perform the same analysis but with a **non-default standard suite of gauge optimizations** - this one toggles the SPAM penalty in addition to varying the spam weight (the default suite just varies the spam weight without any SPAM penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Std Practice:  Iter 1 of 3  (TP) --: \n",
      "  --- Gate Sequence Creation ---\n",
      "   1585 sequences created\n",
      "   Dataset has 3121 entries: 1585 utilized, 0 requested sequences were missing\n",
      "  --- LGST ---\n",
      "  --- Iterative MLGST: [##################################################] 100.0%  1585 gate strings ---\n",
      "  Iterative MLGST Total Time: 1.6s\n",
      "  -- Performing 'Spam 0.0001' gauge optimization on TP estimate --\n",
      "  -- Done gauge optimizing (0.0354259s) -- \n",
      "  -- Performing 'Spam 0.1' gauge optimization on TP estimate --\n",
      "  -- Done gauge optimizing (0.0352201s) -- \n",
      "  -- Performing 'Spam 0.0001+v' gauge optimization on TP estimate --\n",
      "  -- Done gauge optimizing (0.733649s) -- \n",
      "  -- Performing 'Spam 0.1+v' gauge optimization on TP estimate --\n",
      "  -- Done gauge optimizing (0.725161s) -- \n",
      "-- Std Practice:  Iter 2 of 3  (CPTP) --: \n",
      "  --- Gate Sequence Creation ---\n",
      "   1585 sequences created\n",
      "   Dataset has 3121 entries: 1585 utilized, 0 requested sequences were missing\n",
      "  --- Iterative MLGST: [##################################################] 100.0%  1585 gate strings ---\n",
      "  Iterative MLGST Total Time: 5.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: MLGST failed to improve logl: retaining chi2-objective estimate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -- Performing 'Spam 0.0001' gauge optimization on CPTP estimate --\n",
      "  -- Done gauge optimizing (0.315671s) -- \n",
      "  -- Performing 'Spam 0.0001' gauge optimization on CPTP.robust estimate --\n",
      "  -- Done gauge optimizing (0.32058s) --\n",
      "  -- Performing 'Spam 0.1' gauge optimization on CPTP estimate --\n",
      "  -- Done gauge optimizing (0.313397s) -- \n",
      "  -- Performing 'Spam 0.1' gauge optimization on CPTP.robust estimate --\n",
      "  -- Done gauge optimizing (0.313323s) --\n",
      "  -- Performing 'Spam 0.0001+v' gauge optimization on CPTP estimate --\n",
      "  -- Done gauge optimizing (0.932606s) -- \n",
      "  -- Performing 'Spam 0.0001+v' gauge optimization on CPTP.robust estimate --\n",
      "  -- Done gauge optimizing (0.933203s) --\n",
      "  -- Performing 'Spam 0.1+v' gauge optimization on CPTP estimate --\n",
      "  -- Done gauge optimizing (0.933796s) -- \n",
      "  -- Performing 'Spam 0.1+v' gauge optimization on CPTP.robust estimate --\n",
      "  -- Done gauge optimizing (0.923989s) --\n",
      "-- Std Practice:  Iter 3 of 3  (Target) --: \n",
      "  -- Performing 'Spam 0.0001' gauge optimization on Target estimate --\n",
      "  -- Done gauge optimizing (0.00147986s) -- \n",
      "  -- Performing 'Spam 0.1' gauge optimization on Target estimate --\n",
      "  -- Done gauge optimizing (0.00216818s) -- \n",
      "  -- Performing 'Spam 0.0001+v' gauge optimization on Target estimate --\n",
      "  -- Done gauge optimizing (0.396682s) -- \n",
      "  -- Performing 'Spam 0.1+v' gauge optimization on Target estimate --\n",
      "  -- Done gauge optimizing (0.406962s) -- \n"
     ]
    }
   ],
   "source": [
    "results_stdprac_nondefaultgo = pygsti.do_stdpractice_gst(\n",
    "    ds, gs_target, prep_fiducials, meas_fiducials, germs, maxLengths,\n",
    "    modes=\"TP,CPTP,Target\", gaugeOptSuite=\"varySpam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimates:  TP, CPTP, CPTP.robust, Target\n",
      "TP Estimate's gauge optimized gate sets:  Spam 0.0001, Spam 0.1, Spam 0.0001+v, Spam 0.1+v\n"
     ]
    }
   ],
   "source": [
    "print(\"Estimates: \", \", \".join(results_stdprac_nondefaultgo.estimates.keys()))\n",
    "print(\"TP Estimate's gauge optimized gate sets: \", \", \".join(results_stdprac_nondefaultgo.estimates[\"TP\"].goparameters.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll demonstrate how to specify a fully custom set of gauge optimization parameters and how to use a separately-specified target gate set for gauge optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Std Practice:  Iter 1 of 3  (TP) --: \n",
      "  --- Gate Sequence Creation ---\n",
      "   1585 sequences created\n",
      "   Dataset has 3121 entries: 1585 utilized, 0 requested sequences were missing\n",
      "  --- LGST ---\n",
      "  --- Iterative MLGST: [##################################################] 100.0%  1585 gate strings ---\n",
      "  Iterative MLGST Total Time: 1.8s\n",
      "  -- Performing 'myGO' gauge optimization on TP estimate --\n",
      "  -- Done gauge optimizing (0.036927s) -- \n",
      "-- Std Practice:  Iter 2 of 3  (CPTP) --: \n",
      "  --- Gate Sequence Creation ---\n",
      "   1585 sequences created\n",
      "   Dataset has 3121 entries: 1585 utilized, 0 requested sequences were missing\n",
      "  --- Iterative MLGST: [##################################################] 100.0%  1585 gate strings ---\n",
      "  Iterative MLGST Total Time: 5.4s\n",
      "  -- Performing 'myGO' gauge optimization on CPTP estimate --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: MLGST failed to improve logl: retaining chi2-objective estimate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -- Done gauge optimizing (0.318259s) -- \n",
      "  -- Performing 'myGO' gauge optimization on CPTP.robust estimate --\n",
      "  -- Done gauge optimizing (0.314111s) --\n",
      "-- Std Practice:  Iter 3 of 3  (Target) --: \n",
      "  -- Performing 'myGO' gauge optimization on Target estimate --\n",
      "  -- Done gauge optimizing (0.00296187s) -- \n"
     ]
    }
   ],
   "source": [
    "my_goparams = { 'itemWeights': {'gates': 1.0, 'spam': 0.001} }\n",
    "my_gaugeOptTarget = gs_target.depolarize(gate_noise=0.005, spam_noise=0.01) # a guess at what estimate should be\n",
    "results_stdprac_customgo = pygsti.do_stdpractice_gst(\n",
    "    ds, gs_target, prep_fiducials, meas_fiducials, germs, maxLengths,\n",
    "    modes=\"TP,CPTP,Target\", gaugeOptSuite={ 'myGO': my_goparams }, gaugeOptTarget=my_gaugeOptTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimates:  TP, CPTP, CPTP.robust, Target\n",
      "TP Estimate's gauge optimized gate sets:  myGO\n"
     ]
    }
   ],
   "source": [
    "print(\"Estimates: \", \", \".join(results_stdprac_customgo.estimates.keys()))\n",
    "print(\"TP Estimate's gauge optimized gate sets: \", \", \".join(results_stdprac_customgo.estimates[\"TP\"].goparameters.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To finish up, we'll pickle the results for processing in subsequent tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(results, open('tutorial_files/exampleResults.pkl',\"wb\"))\n",
    "pickle.dump(results_TP, open('tutorial_files/exampleResults_TP.pkl',\"wb\"))\n",
    "pickle.dump(results_reduced, open('tutorial_files/exampleResults_reduced.pkl',\"wb\"))\n",
    "pickle.dump(results_stdprac, open('tutorial_files/exampleResults_stdprac.pkl',\"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
